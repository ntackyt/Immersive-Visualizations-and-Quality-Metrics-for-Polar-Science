{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data.\n",
    "Naomi Tack\n",
    "\n",
    "Be sure to set the appropriate folder paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the Segmented Data\n",
    "data_folder_prefix_layer = \"../Seg_Featured_Data/Seg_Featured_\"\n",
    "data_folder_prefix_metric = \"../Seg_Featured_Data_Updated/Seg_Featured_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Cell\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LayerData\n",
    "\n",
    "# Load file names and labels for the processed data\n",
    "\n",
    "with open(\"data_labels.json\", 'r') as json_file:\n",
    "    label_dict = json.load(json_file)\n",
    "\n",
    "\n",
    "file_names = list(label_dict.keys())\n",
    "print(len(file_names))\n",
    "\n",
    "labels = []\n",
    "for fl in file_names:\n",
    "    labels.append(label_dict[fl])\n",
    "print(len(labels))\n",
    "\n",
    "files = [data_folder_prefix_layer+x+\".npy\" for x in file_names]\n",
    "\n",
    "# files = glob.glob(\"Seg_Featured_Data/*.npy\")\n",
    "max_len = 250*250+7\n",
    "num_padded = 0\n",
    "flattened_test_load = np.zeros((max_len, ))\n",
    "print(flattened_test_load.shape)\n",
    "\n",
    "for fl in files:\n",
    "    test_load = np.load(fl, None, allow_pickle=True)\n",
    "    y,x,_ = test_load.shape\n",
    "\n",
    "    # Ignore depth 0\n",
    "    # Keep all Depth 1\n",
    "    #  if the x,y is less than 250x50 pad w 0s\n",
    "    layers_depth = test_load[:,:,1]\n",
    "    # print(layers_depth.shape)\n",
    "    if x < 250:\n",
    "        layers_depth = np.hstack((layers_depth, np.zeros((y, y-x))))\n",
    "        num_padded +=1\n",
    "    if y < 250:\n",
    "        layers_depth = np.hstack((layers_depth, np.zeros((250-y, 250))))\n",
    "    layers_depth = layers_depth.flatten()\n",
    "    # print(layers_depth.shape)\n",
    "\n",
    "\n",
    "    # Get the metrics from the other depths\n",
    "    vl, ct = np.unique(test_load[:,:,5], return_counts=True)\n",
    "    if len(ct)>1:\n",
    "        num_bp = ct[1]\n",
    "    else:\n",
    "        num_bp = 0\n",
    "\n",
    "    # print(\"conected components (mean, std)\", np.mean(test_load[:y//10-1,:x//10-1,2]), np.std(test_load[:y//10-1,:x//10-1,2]))\n",
    "    # print(\"orientaiton (mean, std)\", np.mean(test_load[:y//10-1,:x//10-1,3]), np.std(test_load[:y//10-1,:x//10-1,3]))\n",
    "    # print(\"mean orientaiton (mean, std)\", np.mean(test_load[:250//10-1,:250//10-1,4]), np.std(test_load[:250//10-1,:250//10-1,4]))\n",
    "    # print(\"num breakpoints\",num_bp)\n",
    "    # print(\"dist map (mean, std)\", np.mean(test_load[:,:,6]), np.std(test_load[:,:,6]))\n",
    "\n",
    "    data= np.append(layers_depth, [np.mean(test_load[:y//10-1,:x//10-1,2]), np.std(test_load[:y//10-1,:x//10-1,2]),\n",
    "                                    np.mean(test_load[:y//10-1,:x//10-1,3]), np.std(test_load[:y//10-1,:x//10-1,3]),\n",
    "                                    num_bp,\n",
    "                                    np.mean(test_load[:,:,6]), np.std(test_load[:,:,6])])\n",
    "\n",
    "    # print(data.shape)\n",
    "    flattened_test_load = np.vstack((flattened_test_load, data))\n",
    "\n",
    "print(flattened_test_load.shape,flattened_test_load[0].shape, \"num padded:\", num_padded)\n",
    "\n",
    "flattened_data = flattened_test_load[1:,:]\n",
    "flattened_data.shape, len(labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MetricData\n",
    "\n",
    "# Load file names and labels for the processed data\n",
    "with open(\"../data_labels.json\", 'r') as json_file:\n",
    "    label_dict = json.load(json_file)\n",
    "\n",
    "data_files = glob.glob(data_folder_prefix_metric +\"*\")\n",
    "avalible_files=[]\n",
    "for fl in data_files:\n",
    "    avalible_files.append(fl[len(data_folder_prefix_metric):-len(\".npy\")])\n",
    "\n",
    "print(len(avalible_files), avalible_files[:5])\n",
    "# print(avalible_files[0], list(label_dict.keys())[0])\n",
    "\n",
    "\n",
    "file_names = set.intersection(set(avalible_files), set(list(label_dict.keys())))\n",
    "print(len(file_names))\n",
    "\n",
    "labels = []\n",
    "for fl in file_names:\n",
    "    labels.append(label_dict[fl])\n",
    "print(len(labels))\n",
    "\n",
    "files = [data_folder_prefix_metric+x+\".npy\" for x in file_names]\n",
    "\n",
    "ct, values = np.unique(labels, return_counts=True)\n",
    "print(ct, values, values/values.sum())\n",
    "\n",
    "\n",
    "max_len = len([0]*576+[0]*576+[0]*576+[0]*250*2+[0]*1) \n",
    "flattened_test_load = np.zeros((max_len, ))\n",
    "print(flattened_test_load.shape)\n",
    "\n",
    "for fl in files:\n",
    "    data = np.load(fl, None, allow_pickle=True)\n",
    "    flattened_test_load = np.vstack((flattened_test_load, data))\n",
    "\n",
    "print(flattened_test_load.shape)\n",
    "\n",
    "flattened_data = flattened_test_load[1:,:]\n",
    "flattened_data.shape, len(labels)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
